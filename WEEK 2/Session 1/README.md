# Tools in Data Science (TDS) – Containerization with Podman & Local LLM Deployment

This document summarizes the first session of Week 2 of the Tools in Data Science (TDS) course, which focused on containerization concepts using Podman, running Jupyter Lab and local LLMs inside containers, and building container-based applications.

---

## Session Details
**Course:** Tools in Data Science (TDS)  
**Session:** Week 2 – Session 1  
**Mode:** Recorded Online Session (https://youtu.be/ytGxUwBkcXU) 

**Focus Area:** Containerization, Podman setup, container networking, and containerized AI applications  

---

## Session Overview
The session introduced the fundamentals of containerization and demonstrated how to install and use Podman within a WSL environment. Practical projects included containerizing Jupyter Lab, deploying a local LLM using Olama inside containers, enabling inter-container communication, and building a Flask web application connected to an LLM backend. Troubleshooting networking issues in WSL-based container environments was also discussed.

---

## Topics Covered

### Introduction to Containerization
Overview of container technology, advantages over traditional virtual machines, and the role of containers in modern AI and data science workflows.

---

### Podman Installation in WSL
Step-by-step installation and configuration of Podman inside the WSL environment.

---

### Containerizing Jupyter Lab
Pulling container images, running Jupyter Lab inside containers, and accessing services using port binding.

---

### Running and Managing Containers
Managing container lifecycle operations such as starting, stopping, and monitoring containers.

---

### Port Binding for Container Access
Understanding port mapping to access container services from the host system.

---

### Persisting Data Using Volume Mounting
Using volume mounts to ensure persistent storage for containerized applications.

---

### Containerizing Local LLMs (Olama)
Running local Large Language Models inside containers and interacting with them.

---

### Container Networking
Configuring inter-container communication and networking for multi-container applications.

---

### API Calls Between Containers
Demonstrating communication between containerized services using APIs.

---

### Building a Flask Web App with an LLM Backend
Developing and running a containerized Flask application connected to an LLM backend.

---

### Troubleshooting WSL Networking Issues
Common networking issues in WSL container environments and their solutions.

---

## Key Learnings
- Understood containerization concepts and benefits for AI workflows  
- Learned how to install and configure Podman in WSL  
- Gained experience running Jupyter Lab and LLMs inside containers  
- Learned container networking and inter-container communication concepts  
- Understood how containerized applications can be integrated with AI backends  

---

## Tools & Concepts Introduced
- Containerization  
- Podman  
- Jupyter Lab  
- Volume Mounting  
- Container Networking  
- Flask  
- Local LLM Deployment (Olama)  

---

## Outcome
This session provided practical exposure to container-based development workflows and demonstrated how AI tools, web applications, and supporting services can be deployed and interconnected using container technology.
